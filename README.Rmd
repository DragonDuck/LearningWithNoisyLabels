---
title: "Training a DNN with Noisy Labels"
author: "Jan Sauer"
output: md_document
---

```{r setup, echo=FALSE}
library(ggplot2)
library(pheatmap)

custom_theme <- function(base_size=14, base_family="Helvetica") {
  library(grid)
  library(ggthemes)
  (theme_foundation(base_size=base_size, base_family=base_family)
    + theme(plot.title = element_text(face = "bold",
                                      size = rel(1.2), hjust = 0.5), 
            plot.subtitle = element_text(hjust = 0.5),
            plot.caption = element_text(hjust = 0),
            text = element_text(),
            panel.background = element_rect(colour = NA),
            plot.background = element_rect(colour = NA),
            panel.border = element_rect(colour = NA),
            axis.title = element_text(face = "bold",size = rel(1)),
            axis.title.y = element_text(angle=90,vjust =2),
            axis.title.x = element_text(vjust = -0.2),
            axis.text = element_text(), 
            axis.line = element_line(colour="black"),
            axis.ticks = element_line(),
            # panel.grid.major = element_line(colour="#f0f0f0"),
            panel.grid.major = element_blank(),
            panel.grid.minor = element_blank(),
            legend.key = element_rect(colour = NA),
            legend.position = "right",
            legend.direction = "vertical",
            legend.key.size= unit(0.5, "cm"),
            legend.spacing = unit(1, "cm"),
            legend.title = element_text(face="italic"),
            plot.margin=unit(c(10,5,5,5),"mm"),
            strip.background=element_rect(colour="#f0f0f0",fill="#f0f0f0"),
            strip.text = element_text(face="bold")))}
custom_fill <- function(...){
  library(scales)
  discrete_scale("fill","Publication",manual_pal(values = c(
    "#386cb0","#fdb462","#7fc97f","#ef3b2c","#662506","#a6cee3","#fb9a99",
    "#984ea3","#ffff33")), ...)}
custom_color <- function(...){
  library(scales)
  discrete_scale("colour","Publication",manual_pal(values = c(
    "#386cb0","#fdb462","#7fc97f","#ef3b2c","#662506","#a6cee3","#fb9a99",
    "#984ea3","#ffff33")), ...)}
```

This experiment shows how deep neural networks (DNNs) perform on noisy data. To this end, I trained a convolution DNN (structure: conv-conv-pool-conv-conv-pool-dense) to classify MNIST handwritten digits. However, the labels were randomly perturbed so that a defined fraction of labels was guaranteed to be incorrect. After training, the accuracy (F1-score) of the network was calculated *on the original, unperturbed labels*. This mini-experiment shows that even after significant scrambling, the true accuracy of the network remains nearly unchanged.

```{r, fig.width=4, fig.height=4}
scrambling_results = read.csv(
  file = "ResultsScrambling_mnist_conv.csv", header = TRUE, row.names = NULL)

ggplot(data = scrambling_results, mapping = aes(x = X, y = TrueF1)) + 
  geom_point(size = 4) + geom_line(size = 2) + custom_theme() + 
  coord_fixed() + labs(
    x = "Scrambling Factor", y = "F1-Score", 
    caption = paste0(
       "F1-Score of DNN trained on\n",
       "scrambled labels versus the\n",
       "true labels"))
ggsave("ResultsScrambling_mnist_conv.pdf", width = 4, 
       height = 4, useDingbats = FALSE)
```

I take a look at the confusion matrix of predictions versus true labels. The network clearly favors certain 
```{r}
scrambling = "0.8"
pred_labels = read.csv(
  file = "PredictionsScrambling_mnist_conv.csv", 
  header = TRUE, row.names = 1)
# Hack, this should be done in the python code
colnames(pred_labels) = c(
  "0", "0.25", "0.5", "0.75", "0.8", "0.825",
  "0.85", "0.875", "0.9", "0.95", "1.0", "TrueLabels")

conf_mat = table(
  "Pred" = pred_labels[[scrambling]], 
  "True" = pred_labels$TrueLabels)
conf_lab = conf_mat
diag(conf_mat) = 0
hm_colorScale = colorRampPalette(
  c("#f6f8fb", "#DAE3F0", "#B5C8E1", rep("#8FACD2", 9)))(150)
pheatmap(
  mat = conf_mat, cluster_cols = FALSE, cluster_rows = FALSE, 
  color = hm_colorScale, display_numbers = conf_lab, fontsize_number = 12, 
  legend = FALSE, cellwidth = 30, cellheight = 30, 
  border_color = NA)
pheatmap(
  mat = conf_mat, cluster_cols = FALSE, cluster_rows = FALSE, 
  color = hm_colorScale, display_numbers = conf_lab, fontsize_number = 12, 
  legend = FALSE, cellwidth = 30, cellheight = 30, 
  border_color = NA, filename = sprintf("PredictedLabels_%s.pdf", scrambling))
```


Next, I tested if iterative training, i.e. training a DNN on the outputs of the previous iteration, showed any improvement. To this end, I initially scrambled the labels so that 80% of them are incorrect (iteration 0). The network was trained on the scrambled labels, its accuracy evaluated, and the labels predicted for the entire dataset. These new labels were used to train the next iteration of the DNN. There seems to be no benefit to iterative training. While the accuracy still increases slowly, the time cost of training over so many iterations far outweighs the benefits of a minor increase in accuracy.
```{r, fig.width=3, fig.height=4}
iteration_results = read.csv(
  file = "ResultsIterative_mnist_conv_0.8.csv", 
  header = TRUE, row.names = NULL)
iteration_results$X = as.factor(iteration_results$X)
iteration_results$TextLoc = iteration_results$TrueF1 - 0.08
iteration_results$Text = round(iteration_results$TrueF1, 2)

ggplot(data = iteration_results, mapping = aes(x = X, y = TrueF1)) + 
  geom_col() + geom_text(mapping = aes(x = X, y = TextLoc, label = Text), 
                         color = "White", fontface = "bold", size = 3) +
  custom_theme() + ylim(c(0, 1)) + coord_flip() + 
  labs(
    x = "Iteration", y = "F1-Score", 
    caption = paste0(
       "F1-Score of DNN iteratively\n",
       "trained on its own output,\n",
       "starting with scrambled\n",
       "labels, versus the true labels"))
ggsave("ResultsIterative_mnist_conv_0.8.pdf", width = 3, 
       height = 4, useDingbats = FALSE)
```

Testing this for a higher scrambling degree of 82.5% shows that iteration still offers no benefit. However, the network saturates at a lower accuracy.
```{r, fig.width=3, fig.height=4}
iteration_results = read.csv(
  file = "ResultsIterative_mnist_conv_0.825.csv", 
  header = TRUE, row.names = NULL)
iteration_results$X = as.factor(iteration_results$X)
iteration_results$TextLoc = iteration_results$TrueF1 - 0.08
iteration_results$Text = round(iteration_results$TrueF1, 2)

ggplot(data = iteration_results, mapping = aes(x = X, y = TrueF1)) + 
  geom_col() + geom_text(mapping = aes(x = X, y = TextLoc, label = Text), 
                         color = "White", fontface = "bold", size = 3) +
  custom_theme() + ylim(c(0, 1)) + coord_flip() + 
  labs(
    x = "Iteration", y = "F1-Score", 
    caption = paste0(
       "F1-Score of DNN iteratively trained on\n",
       "its own output starting with scrambled\n",
       "labels, versus the true labels"))
ggsave("ResultsIterative_mnist_conv_0.825.pdf", width = 3, 
       height = 4, useDingbats = FALSE)
```

